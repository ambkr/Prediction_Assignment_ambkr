---
title: "Practical Machine Learning - Prediction Assignment"
author: "Alexander Baker"
date: '2018-01-06'
output: md_document
---

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.
```

## Acquiring the data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 

If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Loading the required libraries and data

Firstly, we will download the data from the website load it into two data frames: the training dataset and the testing dataset with 20 observations that will be evaluated on Coursera. Note, the testing dataset has had the "classe" variable removed.

```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)

# downloading the training data
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
              destfile = "./pml-training.csv", method = "curl")

# loading the training data
pml_training <- read.csv("./pml-training.csv", na.strings=c("NA","#DIV/0!",""))

# downloading the testing data
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
              destfile = "./pml-testing.csv", method = "curl")

# loading the testing data
pml_testing <- read.csv("./pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```

## Cleaning the data

We remove the columns that contain NA and features that are not in the testing dataset, leaving 53 features.

``` {r}
remaining_features <- names(pml_testing[,colSums(is.na(pml_testing)) == 0])[8:59]

pml_training_cleaned <- pml_training[,c(remaining_features,"classe")]
pml_testing_cleaned <- pml_testing[,c(remaining_features,"problem_id")]

dim(pml_training_cleaned); dim(pml_testing_cleaned);
```

## Partitioning data into testing and training sets

By using the recommendation of splitting data into 60% training and 40% testing sets for medium size studies, we can estimate out of sample error in the testing set. This is not the same as the FINAL testing set evaluated in COURSERA.

```{r}
set.seed(1969)

sort_train_test <- createDataPartition(pml_training_cleaned$classe, p=0.6, list=FALSE)
training <- pml_training_cleaned[sort_train_test,]
testing <- pml_training_cleaned[-sort_train_test,]

dim(training)
dim(testing)
```

## Comparing Models

In order to compare the suitability of different models, we will try at least two in order to find one that works well. 
We will begin by building a tree model (using the 'tree' package), followed by a random forests model (using the 'randomForests' package).

## Building a Decision Tree Model

```{r}
library(tree)
set.seed(1969)
training_tree <- tree(classe ~ ., data=training)
summary(training_tree)
```
We can see that the model has 17 terminal nodes.

Plotting is also helpful

```{r}
plot(training_tree)
text(training_tree,pretty=0, cex=0.9)
```

We can also do a plot to visualize the terminal nodes. We can see from that plot that the deviance decreases as the te size of the tree gets smaller.

```{r}
visualize_tree=cv.tree(training_tree,FUN=prune.misclass)
visualize_tree
plot(visualize_tree)
```

We can also calculate the out of sample error rate by evaluating the testing data.
```{r}
tree_predictions=predict(training_tree,testing,type="class")
prediction_matrix = with(testing,table(tree_predictions,classe))
sum(diag(prediction_matrix))/sum(as.vector(prediction_matrix)) ## calculating for accuracy in tree model
```

Unfortuntately, we can see the mediocre performance of the tree model we've built when applied to the testing dataset. For this reason, we will now build a random forests model.

## Building a Random Forest Model

The random forest model amplifies the effectiveness of many weak tree predictors. We are using the 'randomForest' package.

```{r}
set.seed(1969)
training_rf=randomForest(classe~.,data=training,ntree=100, importance=TRUE)
training_rf
```
We can see that the OOB estimate of error rate in-sample is 80%.

We will produce a variable importance plot to visualize which variables are having a higher impact on the prediction.
```{r}
#plot(training_rf, log="y")
varImpPlot(training_rf)
```

Let's apply this model on the testing dataset and determine the suitability of the model.

``` {r}
tree_predictions=predict(training_rf,testing,type="class")
prediction_matrix = with(testing,table(tree_predictions,classe))
sum(diag(prediction_matrix))/sum(as.vector(prediction_matrix)) # calculating for accuracy in random forests model
```

We can see that the superb accuracy (better than 0.99) means our random forests model is ideal.

We will use it to evaluate the Coursera Quiz testing set

## Evaluating Coursera Quiz testing set
```{r}
evaluating_quiz_testing_set <- predict(training_rf, pml_testing)
evaluating_quiz_testing_set
```

These predictions pass the Coursera Quiz without error.


